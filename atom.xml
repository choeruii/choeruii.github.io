<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stay Hungary, Stay Foolish</title>
  
  
  <link href="https://choeruii.github.io/atom.xml" rel="self"/>
  
  <link href="https://choeruii.github.io/"/>
  <updated>2021-10-06T13:02:46.019Z</updated>
  <id>https://choeruii.github.io/</id>
  
  <author>
    <name>Eason Zhao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Lecture 6： 生成对抗网络（GAN）</title>
    <link href="https://choeruii.github.io/2021/10/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%EF%BC%88GAN%EF%BC%89/"/>
    <id>https://choeruii.github.io/2021/10/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%EF%BC%88GAN%EF%BC%89/</id>
    <published>2021-10-06T12:49:59.644Z</published>
    <updated>2021-10-06T13:02:46.019Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Lecture-6：-生成对抗网络（GAN）"><a href="#Lecture-6：-生成对抗网络（GAN）" class="headerlink" title="Lecture 6： 生成对抗网络（GAN）"></a>Lecture 6： 生成对抗网络（GAN）</h3><iframe data="F:\大学事务\三年级秋季\李宏毅_机器学习\笔记\Lecture 6_生成对抗网络(GAN).pdf" type="application/pdf" width="100%" height="877px"> </iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Lecture-6：-生成对抗网络（GAN）&quot;&gt;&lt;a href=&quot;#Lecture-6：-生成对抗网络（GAN）&quot; class=&quot;headerlink&quot; title=&quot;Lecture 6： 生成对抗网络（GAN）&quot;&gt;&lt;/a&gt;Lecture 6： 生成对抗网络（GA</summary>
      
    
    
    
    
    <category term="机器学习(李宏毅)随缘学习笔记" scheme="https://choeruii.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85-%E9%9A%8F%E7%BC%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习任务攻略（前期如何做作业）</title>
    <link href="https://choeruii.github.io/2021/08/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E6%94%BB%E7%95%A5/"/>
    <id>https://choeruii.github.io/2021/08/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E6%94%BB%E7%95%A5/</id>
    <published>2021-08-28T09:59:27.122Z</published>
    <updated>2021-10-06T12:49:10.014Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Lecture-2：机器学习任务攻略（前期如何做作业）"><a href="#Lecture-2：机器学习任务攻略（前期如何做作业）" class="headerlink" title="Lecture 2：机器学习任务攻略（前期如何做作业）"></a>Lecture 2：机器学习任务攻略（前期如何做作业）</h3><p>Training Data$\Large \Rightarrow$Training（Lecture 1：三个步骤）$\Large \Rightarrow$<strong>Testing data</strong></p><p><img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210830192406468.png" alt="image-20210830192406468" style="zoom:60%;" /></p><h4 id="1-从-loss-on-training-data-着手"><a href="#1-从-loss-on-training-data-着手" class="headerlink" title="1.从 loss on training data 着手"></a><em>1.从 loss on training data 着手</em></h4><h4 id="1-1Model-Bias"><a href="#1-1Model-Bias" class="headerlink" title="1.1Model Bias"></a>1.1Model Bias</h4><p>模型过于简单或者与实际相差过多，无论如何迭代，loss值无法降低。需要让模型更加flexible。在课程里层数越多模型越有弹性。</p><p><img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210830130423703.png" alt="image-20210830130423703" style="zoom:60%;" /></p><h4 id="1-2优化问题（Optimization-Issue）"><a href="#1-2优化问题（Optimization-Issue）" class="headerlink" title="1.2优化问题（Optimization Issue）"></a>1.2优化问题（Optimization Issue）</h4><h5 id="寻找loss陷入局部最优解"><a href="#寻找loss陷入局部最优解" class="headerlink" title="寻找loss陷入局部最优解"></a>寻找loss陷入局部最优解</h5><p><img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210830130529660.png" alt="image-20210830130529660" style="zoom:60%;" /></p><p>关于两者的比较和判断，介绍了文章<a href="[[1512.00338\] Population imbalance in the extended Fermi-Hubbard model (arxiv.org">Population imbalance in the extended Fermi-Hubbard model</a>](<a href="https://arxiv.org/abs/1512.00338))当两个网络A、B，A在B的基础上有更多的层数，但是在任务上A的loss要比B大，这说明A网络的Optimization没有做好。">https://arxiv.org/abs/1512.00338))当两个网络A、B，A在B的基础上有更多的层数，但是在任务上A的loss要比B大，这说明A网络的Optimization没有做好。</a></p><p>从对比中，我们可以获得更确切的认知；我们可以从较为浅的model开始着手；如果更深的网络并没有得到更小的loss，那么该网络有optimization issue</p><p>当我们在training data上得到良好的loss，我们就可以着手在testing data上降低loss</p><h4 id="2-从-loss-on-testinging-data-着手"><a href="#2-从-loss-on-testinging-data-着手" class="headerlink" title="2.从 loss on testinging data 着手"></a><strong><em>2.从 loss on testinging data 着手</em></strong></h4><h4 id="2-1-overfitting-过拟合"><a href="#2-1-overfitting-过拟合" class="headerlink" title="2.1 overfitting 过拟合"></a>2.1 overfitting 过拟合</h4><ul><li>增加training data（作业里不行）</li><li>Data Augmentation，根据自己对任务的理解，人为创造出一些新的数据。例如：图像识别训练中可以把训练图片左右翻转，裁剪获得新的训练数据</li><li>给予模型一定限制，使其不那么flexible<ul><li>更少的参数</li><li>更少的features</li><li>Early stopping、Regularization、Dropout（Lecture 4）</li></ul></li></ul><p>Bias-Complexity Trade-off：模型复杂的程度（或曰模型的弹性）——function比较多，随着复杂度增加，training的loss越来越小，然而testing的loss是一个凹状的曲线（先小后大）。</p><p><img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210830194242557.png" alt="image-20210830194242557" style="zoom:50%;" /></p><blockquote><p>机器学习比赛（例如Kaggle）分为两个Leaderboard：public和private（A、B榜），在两个测试集上的分数的差别过大在于model不够鲁棒。换言之，在公用数据集上达到较高的准确率，不见得在落地使用上能完全实现其测试的level（骗骗麻瓜的商业蜜口）。</p><p>每日限制上传次数主要是为了防止各位水模型不断test公用数据集刷分数（无意义~~）</p></blockquote><h5 id="Cross-Validation-交叉验证"><a href="#Cross-Validation-交叉验证" class="headerlink" title="Cross Validation 交叉验证"></a>Cross Validation 交叉验证</h5><p>把training data分成两半：training data和validation data。 如何分呢？可以随机分；另外，可以用<strong>N-折交叉验证（N-fold Cross Validation）</strong></p><p><img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210830194316768.png" alt="image-20210830194316768" style="zoom:120%;" /></p><h4 id="2-2-mismatch"><a href="#2-2-mismatch" class="headerlink" title="2.2 mismatch"></a>2.2 mismatch</h4><p>也可以认为是一种overfitting</p><p>Mismatch表示训练数据和测试数据的<strong>分布（distributions）</strong>不一致。</p><p>（HW11针对这个问题）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Lecture-2：机器学习任务攻略（前期如何做作业）&quot;&gt;&lt;a href=&quot;#Lecture-2：机器学习任务攻略（前期如何做作业）&quot; class=&quot;headerlink&quot; title=&quot;Lecture 2：机器学习任务攻略（前期如何做作业）&quot;&gt;&lt;/a&gt;Lectu</summary>
      
    
    
    
    
    <category term="机器学习(李宏毅)随缘学习笔记" scheme="https://choeruii.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85-%E9%9A%8F%E7%BC%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Lecture 1 &amp;&amp; Lecture 2:机器学习/深度学习基本概念简介</title>
    <link href="https://choeruii.github.io/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%AE%80%E4%BB%8B/"/>
    <id>https://choeruii.github.io/2021/08/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%AE%80%E4%BB%8B/</id>
    <published>2021-08-25T04:32:56.345Z</published>
    <updated>2021-10-06T12:48:57.307Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Lecture-1-Introduction-of-ML-DL"><a href="#Lecture-1-Introduction-of-ML-DL" class="headerlink" title="Lecture 1: Introduction of ML/DL"></a>Lecture 1: Introduction of ML/DL</h3><h4 id="机器学习基本概念简介"><a href="#机器学习基本概念简介" class="headerlink" title="机器学习基本概念简介"></a>机器学习基本概念简介</h4><p><em>Machine Learning $\approx$ Looking for Function</em>——机器学习就是让机器（程序）具备找一个函数的能力。</p><p><em>Different types of Funtions：</em></p><ul><li>Regression（回归）——连续。最终得到标量（scalar）</li><li><p>Classification（分类）——离散。得到一个选择（options/classes）</p></li><li><p>除此两大任务外，还有<strong>Structured Learning</strong>：让机器不仅学会分类或者实现预测任务，而且可以创造特定的“有结构”的物体，譬如文章、图像等。</p></li></ul><p><em>机器学习如何找到这个函数？（三个步骤）</em></p><ul><li><h5 id="Function-with-Unknown-Parameters："><a href="#Function-with-Unknown-Parameters：" class="headerlink" title="Function with Unknown Parameters："></a>Function with Unknown Parameters：</h5><p>譬如$y = b + wx_1$，该假设方程是基于<strong>domain knowledge</strong>（领域知识）各种定义：</p><ul><li><strong>Model</strong>：带有未知的参数(Parameters)的函数（function）。</li><li>$x_1$是<strong>feature</strong>，$w$是<strong>weight</strong>，$b$是<strong>bias</strong>，后两个未知参数基于数据(data)学习得到。</li></ul></li><li><h5 id="Define-Loss-from-Training-Data："><a href="#Define-Loss-from-Training-Data：" class="headerlink" title="Define Loss from Training Data："></a>Define Loss from Training Data：</h5><ul><li>Loss，即损失函数，一个仅带有函数未知的参数的方程，记作$L(b,w)$</li><li>Loss的值体现了函数的一组参数的设定的优劣</li><li>通过训练资料来计算loss = |估测值 - 真正值|，<strong>Label</strong>指的就是正确的数值$\hat{y}$，$e_i = |y - \hat{y}|,i = 1,2,..,n$，所以。<strong>Loss</strong>：$L = \Large\frac{1}{N}\sum_n^{i=1}e_i$。其中，差值$e$的有不同的计算方法，如上采用直接做差得绝对值（Mean Absolute Error：MAE），还有$e = (y-\hat{y})^2$，即Mean Square Error：MSE。选择哪一种方法衡量$e$取决于我们的需求以及对于task的理解。</li><li>我们枚举不同参数组合（$w,b$）通过计算Loss值画出等高线图：<strong>Error Surface</strong></li><li>如果$y$和$\hat{y}$都是概率==&gt;<strong>Cross-entropy</strong>：交叉熵</li><li>loss函数自定义设定，如果有必要的话，loss函数可以output负值</li></ul></li><li><h5 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h5><ul><li><p>$w^<em>,b^</em> = arg\space \underset{w,b}{min}L $</p></li><li><p>为了实现上述任务（找到$w,b$使得$L$最小）,通常采用梯度下降法（<strong>Gradient Descent</strong>）。譬如：隐去其中一个参数<img src="https://i.loli.net/2021/08/25/2uQKMwqytpTIYHx.png" alt="image-20210824114633124.png" style="zoom:50%;" />从而得到一个$w-Loss(L)$的数值曲线，记作$L(w)$</p><ul><li><p><strong>随机</strong>选取一个初始值：$w_0$</p></li><li><p>计算：$\Large \frac{\partial L}{\partial w}|_{w=w_0}$，该点位置在Error Surface的切线斜率：若负值（Negative），左高右低=&gt;$w$右移$\eta$使得$Loss$变小；若正值（Positive），左底右高=&gt;$w$左移$\eta$使得$Loss$变小。斜率大=&gt;步伐$\eta$跨大一些；斜率小=&gt;步伐$\eta$跨小一些。$w_1 \leftarrow w_0 - \eta \large \frac{\partial L}{\partial w}|_{w=w_0}$</p><p><strong>$\eta$</strong> : learning rate学习率，属于<strong>hyper parameters</strong>：超参数，自己设定，决定更新速率。</p></li><li><p>不断迭代更换$w$</p><p><strong>“假”问题</strong>：囿于局部最优解local minimal，忽略了实际的最优解global minima（不过并非梯度下降法的真正痛点）</p></li></ul></li><li><p>类似的，将单参数随机梯度下降法推广到两参数上：$w^<em>,b^</em> = arg\space \underset{w,b}{min}L $</p><p><img src="https://i.loli.net/2021/08/25/f8Jz5hgpKE9j6wb.png" alt="image-20210824153357084.png" style="zoom:67%;" /></p><p>确定<strong>更新方向：$(- \eta \large \frac{\partial L}{\partial w},- \eta \large \frac{\partial L}{\partial b})$</strong>，$\eta$为学习率</p><p>总结来说，基本步骤如下</p></li></ul></li></ul><p><img src="https://i.loli.net/2021/08/25/IJfdrzTQSlwobL5.png" alt="image-20210824153852773.png" style="zoom:67%;" /></p><p>以上三步是机器学习最为基本的框架。基于此，还需要理解任务，摸索数据变化规律==&gt;修改模型（model）</p><hr><h4 id="深度学习基本概念简介"><a href="#深度学习基本概念简介" class="headerlink" title="深度学习基本概念简介"></a>深度学习基本概念简介</h4><p>在以上基本步骤上进行深层的探讨。</p><h5 id="Step-1：定义含有未知参数的函数"><a href="#Step-1：定义含有未知参数的函数" class="headerlink" title="Step 1：定义含有未知参数的函数"></a>Step 1：定义含有未知参数的函数</h5><p>线性函数被首先考虑。线性模型（Linear Model）过于简单，无论参数组合如何可能总是无法完全拟合任务的Model，这里说明Linear Model具有<em>severe limitation</em>，这种局限被称之为<strong>Model Bias</strong>。于是我们需要更为复杂的函数。</p><p><img src="https://i.loli.net/2021/08/25/rqWxcShlPG93YEI.png" alt="image-20210824183803053.png" style="zoom: 67%;" /></p><p>这里类似于使用<strong>阶跃函数</strong>的组合来表示分段函数，<font color ='red'>red curve</font>= 1 + 2 + 3  + 0（常数项）,这里归纳出一个常见的结论：分段函数$All\space Piecewise\space Linear\space Curves = constant$(常数项) + <img src="https://i.loli.net/2021/08/25/Fpa8JmIR2954kzu.png" alt="image-20210824184212031.png" style="zoom:67%;" /></p><p>那么，对于$Beyond\space Piecewise\space Linear\space Curves$（这也是我们常见的一般函数的曲线），我们使用许多多不一样的小线段去“逼近”连续的这条曲线：</p><p><img src="https://i.loli.net/2021/08/25/xqeEb5zpsMktcUm.png" alt="image-20210824184452998.png" style="zoom:67%;" /></p><p>为了表示这样一个蓝色的函数（小线段）<img src="https://i.loli.net/2021/08/25/FEBpQxw3nduf6eR.png" alt="image-20210824185111847.png" style="zoom:67%;" />（被称之<strong>Hard Sigmoid</strong>），这里用一个常见的指数函数来逼近——<strong>Sigmoid Function</strong></p><script type="math/tex; mode=display">y = c \large \frac{1}{1 + e^{-(b+wx_1)}}= c·sigmoid(b+wx_1)</script><p>通过调整$w,b,c$，一组参数组合可以得到不同逼近的小线段👇</p><p><img src="https://i.loli.net/2021/08/25/CM5G4TEDniXfwbk.png" alt="image-20210824185136913.png" style="zoom:80%;" /></p><p>这个引入超级棒！！由上易知，一个连续的复杂的函数曲线可以被分解成许多离散的小线段（<strong>Hard Sigmoid</strong>）和一个常数项的线性相加，然后每个小线段被一个三参数的<strong>Sigmoid Function</strong>所逼近。下图的函数曲线可以表示为一个含有10个未知参数的mode：</p><p><img src="https://i.loli.net/2021/08/25/LDXyeMzpQSwIJmR.png" alt="image-20210824190018310.png" style="zoom: 67%;" /></p><p>从而，可以产生一个从简单-&gt;复杂、单一-&gt;多元的函数模式。新的模型包含更多的特征。</p><script type="math/tex; mode=display">y=b+wx_1    \Rightarrow y = b + \underset{i}{\sum}c_i sigmoid(b_i+w_ix_1)</script><p>由（2）式，考虑到多特征因素，进一步扩展得</p><script type="math/tex; mode=display">y = b + \underset{j}{\sum}w_jx_j \Rightarrow y = b + \underset{i}{\sum}c_i sigmoid(b_i+\underset{j}{\sum}w_{ij}x_1)</script><p>其中$i$表示$i^{th}$个$Sigmoid$函数（模型的基函数个数），$x_j$表示一个函数中不同的特征或者预测的数据长度，，$w_j$表示对应特征权值。</p><p><img src="https://i.loli.net/2021/08/25/PSRrlUA7pM21e5I.png" alt="image-20210824230821610.png" style="zoom:67%;" /></p><p>总结：在通用的机器学习教程中，$sigmoid$函数普遍被视作一款常见的激活函数，在本课程中，从代表任务模型的非线性函数出发—&gt;极限：分段的线性函数组合—&gt;不同性质/特征的$sigmoid$函数逼近小分割的线性函数。如上图所示，我们有三个激活函数（$sigmoid \space function$）以及输出的一个方程组（矩阵/向量相乘表示），这里基本上可以视为一个具有三个神经元的全连接的一层神经网络。</p><script type="math/tex; mode=display">[r_1,r_2,r_3]^T = [b_1,b_2,b_3]^T + \begin{bmatrix}w_{11},w_{12},w_{13}\\w_{21},w_{22},w_{23}\\w_{31},w_{32},w_{33} \end{bmatrix}·[x_1,x_2,x_3]^T</script><p>总之，</p><script type="math/tex; mode=display">r = \mathbb {b} +W·\mathbb{x}</script><p>接下来，将该方程组$r$通过激活函数输出向量$a$，这里</p><script type="math/tex; mode=display">a = \sigma(r)</script><p><img src="https://i.loli.net/2021/08/25/Q7zeoTXxCrWVbj3.png" alt="image-20210824233230068.png" style="zoom:80%;" /></p><p>由(5)、(6)得</p><script type="math/tex; mode=display">由 a= \sigma(\mathbb{b} + W·\mathbb{x}): \\\Rightarrow y =  b + [c_1,c_2,c_3]·\sigma(\mathbb{b} + W·\mathbb{x})\Rightarrow y = b + \mathbb{c}^T\sigma(\mathbb{b} + W·\mathbb{x})</script><p>注意，$\sigma$中的$\mathbb{b}$是向量，外面的$b$是数值，结果$y$也是数值（标量）。</p><p>在该例子中，$\mathbb{x}$表示特征，$\mathbb{c}、\mathbb{b}、W、b$为未知参数。为了把未知参数统一起来处理，我们进行如下泛化，比方说，$\theta_1 = [c_1,b_1,w_{11},w_{12},w_{13},b]^T$</p><p><img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210828163721308.png" alt="image-20210828163721308" style="zoom: 60%;" /></p><p>$\mathbb{\theta}$是一个很长的向量，里面的第一个向量为$\theta_1$，以此类推。只要是未知参数都统称在$\theta$内。</p><p>在参数很少的时候，可以直接穷举参数组合，寻找最优解；但是当机器学习问题中的参数较多时，梯度下降法更为合理。隐含层神经元节点个数（$sigmoid$函数个数）自己决定，其本身个数数值也为超参数之一。</p><h5 id="Step-2：确定loss函数"><a href="#Step-2：确定loss函数" class="headerlink" title="Step 2：确定loss函数"></a>Step 2：确定loss函数</h5><ul><li>loss是一个未知参数的函数：$L(\mathbb{\theta})$</li><li>loss衡量一组参数值表示模型效果优劣</li></ul><p><img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210828165051628.png" alt="image-20210828165051628" style="zoom:67%;" /></p><p>同以上介绍的步骤无区别。</p><h5 id="Step-3：Optimization"><a href="#Step-3：Optimization" class="headerlink" title="Step 3：Optimization"></a>Step 3：Optimization</h5><p>新模型的的optimization步骤和之前介绍的无任何区别。对于$\mathbb{\theta}=[\theta_1,\theta_2,\theta_3…]^T$</p><ul><li><p>随机选取初始值$\mathbb{\theta}^0$，<strong>gradient</strong>梯度记为$\large\mathbb{\mathcal{g}}=[\frac{\partial L}{\partial \theta_1}_{|\mathbb{\theta}=\mathbb{\theta}^0},\frac{\partial L}{\partial \theta_2}_{|\mathbb{\theta}=\mathbb{\theta}^0},…]^T$，可简化为$\mathbb{\mathcal{g}}=\nabla L(\mathbb{\theta}^0)$向量长度=参数个数。</p></li><li><p>更新参数👇($\eta$当然是学习率啦)</p><script type="math/tex; mode=display">\mathbb{\theta}=[\theta_1^1,\theta_2^1,...]^T \leftarrow \mathbb{\theta}=[\theta_1^0,\theta_2^0,...]^T - [\textcolor{red}\eta\frac{\partial L}{\partial \theta_1}_{|\mathbb{\theta}=\mathbb{\theta}^0},\textcolor{red}\eta\frac{\partial L}{\partial \theta_2}_{|\mathbb{\theta}=\mathbb{\theta}^0},...]^T\\ \mathbb{\theta}^1 \leftarrow \mathbb{\theta}^0 - \textcolor{red}\eta  \mathbb{\mathcal{g}}</script><p>不断迭代$\mathbb{\theta}^2 \leftarrow \mathbb{\theta}^1 - \textcolor{red}\eta  \mathbb{\mathcal{g}},\mathbb{\theta}^3 \leftarrow \mathbb{\theta}^2 - \textcolor{red}\eta  \mathbb{\mathcal{g}},…$，直到找到不想做或者梯度最后是zero vector（后者不太可能）。 </p></li></ul><p>实际上在做梯度下降的时候，我们要把数据$N$分成若干<strong>Batch</strong>（称之为<strong>批量</strong>）,如何分？随便分。原先是把所有data拿来算一个loss，现在是在一个Batch上算loss，那么对于$B_1,B_2,…$我们可以得到$L^1,L^2,…$</p><p><img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210828171640225.png" alt="image-20210828171640225" style="zoom:67%;" /></p><p>把所有batch算过一次，称之为一个<strong>epoch</strong>：1 <strong>epoch</strong>  = see all the batches once。以上即为<strong>批量梯度下降</strong>。注意区别：一次update指的是每次更新一次参数，而把所有的Batch看过一遍则是epoch。</p><p>另外，<strong>Batch Size</strong>大小也是一个超参数。</p><h5 id="对模型做更多的变形："><a href="#对模型做更多的变形：" class="headerlink" title="对模型做更多的变形："></a>对模型做更多的变形：</h5><p>$Sigmoid \rightarrow ReLU$：<strong>Rectified Linear Unit（ReLU）</strong>：$c·max(0,b+wx_1)$曲线。不同的是，我们需要两个$ReLU$曲线才能合成一个<strong>Hard Sigmoid</strong>函数曲线（蓝色的小线段）。无论是$Sigmoid$还是$ReLU$都是<strong>激活函数（Activation Function）</strong>。</p><p>上面的长篇大论仅仅讲述了一层神经网络是如何搭建的，那么多层神经网络的耦合（或者是逐步构建隐藏层）$\rightarrow$<strong>深度学习（Deep Learning</strong>）。老师表示这里的层数也是个超参数哦。层数越多，参数越多。</p><p>同一层好多个激活函数（Neruon）就是一个hidden layer，多个hidden layer组成了Neural Network。这一整套技术就是deep learning。</p><p>之后的神经网络层数越来越多（AlexNet、GoogLeNet等等）那么为什么是<strong>深</strong>度学习而不是<strong>宽（肥）</strong>度学习？另外，当层数变多了，就会<strong>overfitting（过拟合）</strong>。这些是我们之后课程要讨论的问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Lecture-1-Introduction-of-ML-DL&quot;&gt;&lt;a href=&quot;#Lecture-1-Introduction-of-ML-DL&quot; class=&quot;headerlink&quot; title=&quot;Lecture 1: Introduction of ML/</summary>
      
    
    
    
    
    <category term="机器学习(李宏毅)随缘学习笔记" scheme="https://choeruii.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85-%E9%9A%8F%E7%BC%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://choeruii.github.io/2021/01/26/%E5%B8%B8%E7%94%A8%E7%9A%84%E5%85%83%E8%83%9E%E8%87%AA%E5%8A%A8%E6%9C%BA/"/>
    <id>https://choeruii.github.io/2021/01/26/%E5%B8%B8%E7%94%A8%E7%9A%84%E5%85%83%E8%83%9E%E8%87%AA%E5%8A%A8%E6%9C%BA/</id>
    <published>2021-01-26T01:36:31.501Z</published>
    <updated>2021-01-26T12:31:16.342Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title:常用的元胞自动机<br>tags:”元胞自动机”</p><h2 id="mathjax-true"><a href="#mathjax-true" class="headerlink" title="mathjax:true"></a>mathjax:true</h2><p>​        在元胞自动机是由空间上各项同性的一系列元胞所组成，是在有限元胞自动机基础上发展起来的，用于模拟和分析几何空间内的各种现象。</p><h4 id="2-2-1-典型的元胞自动机"><a href="#2-2-1-典型的元胞自动机" class="headerlink" title="2.2.1 典型的元胞自动机"></a>2.2.1 典型的元胞自动机</h4><p>​        在元胞自动机的发展过程中，科学家们构造了各种各样的元胞自动机模型。其中，以下几个典型模型对元胞自动机的理论方法的研究起到了极大的推动作用，因此，它们又被认为是元胞自动机发展历程中的几个里程碑。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;p&gt;title:常用的元胞自动机&lt;br&gt;tags:”元胞自动机”&lt;/p&gt;
&lt;h2 id=&quot;mathjax-true&quot;&gt;&lt;a href=&quot;#mathjax-true&quot; class=&quot;headerlink&quot; title=&quot;mathjax:true&quot;&gt;&lt;/a&gt;mathjax</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>什么是元胞自动机</title>
    <link href="https://choeruii.github.io/2020/10/18/%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E5%8A%A8%E6%9C%BA/"/>
    <id>https://choeruii.github.io/2020/10/18/%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E5%8A%A8%E6%9C%BA/</id>
    <published>2020-10-18T14:34:08.961Z</published>
    <updated>2021-01-25T09:31:14.678Z</updated>
    
    <content type="html"><![CDATA[<p>​        <strong>元胞自动机</strong>(<code>Cellular Automata</code>，简称CA，也有人译为细胞自动机、点格自动机、分子自动机或单元自动机)。是一时间和空间都离散的动力系统。散布在规则格网<code>(Lattice Grid)</code>中的每一元胞(Cell)取有限的离散状态，遵循同样的作用规则，依据确定的局部规则作同步更新。大量元胞通过简单的相互作用而构成动态系统的演化。不同于一般的动力学模型，元胞自动机不是由严格定义的物理方程或函数确定，而是用一系列模型构造的规则构成。凡是满足这些规则的模型都可以算作是元胞自动机模型。因此，元胞自动机是一类模型的总称，或者说是一个方法框架。其特点是时间、空间、状态都离散，每个变量只取有限多个状态，且其状态改变的规则在时间和空间上都是局部的。</p><p><strong>1. 自动机</strong></p><p>  自动机(Automaton)通常指不需要人们逐步进行操作指导的设备(夏培肃，1984)。例如，全自动洗衣机可按照预先安排好的操作步骤作自动地运行;现代计算机能自动地响应人工编制的各种编码指令。完成各种复杂的分析与计算;机器人则将自动控制系统和人工智能结合，实现类人的一系列活动。另一方面，自动机也可被看作为一种离散数字动态系统的数学模型。例如，英国数学家<code>A.M.Turing</code>于1936年提出的图灵机就是一个描述计算过程的数学模型<code>(A.M.Turing,1936)</code>。它是由一个有限控制器、一条无限长存储带和一个读写头构成的抽象的机器，并可执行如下操作:</p><ul><li>读写头在存储带上向左移动一格;</li><li>读写头在存储带上向右移动一格;</li><li>在存储的某一格内写下或清除一符号;</li><li>条件转移。</li></ul><p>​        图灵机在理论上能模拟现代数字计算机的一切运算，可视为现代数字计算机的数学模型。实际上，一切”可计算”函数都等价于图灵机可计算函数，而图灵机可计算函数类又等价于一般递归函数类。<br>      根据存储带是否有限，可将自动机划分为有限带自动机<code>(Finite Automaton)</code>和无限带自动机<code>(Infinite Automaton)</code>。由于图灵机有无限长的存储带，所以为一种无限带自动机。有限带自动机常用作数字电路的数学模型，也用来描述神经系统和算法;而无限带自动机主要用来描述算法，也用来描述繁殖过程 (如细胞型自动机和网络型自动机)。<br>有限自动机是一种控制状态有限、符号集有限的自动机，是一种离散输入输出系统的数学模型。可将有限自动机设想成由一条划分为许多方格的输入带和一个控制器组成的机器:在输入带的每一个小格中可以容纳一个符号，这些符号取自一个有限符号集S-控制器具有有限个可能状态(构成集合Q)。并在每一时刻仅处于其中的一个状态q;控制器有一个读入头，可以从输入带中读入符号;时间是离散的，初始时控制器处在状态;控制器的功能是根据其当前状态g和读入头从输入带上得到的符号a,来确定控制器的下一时刻的状态实现从状态q到状态q’，实现从状态q到状态q’的转移，并将读入头右移一格。控制器另一功能是识别终止状态(它们构成Q的一个子集F)，也可将该识别功能视为有限自动机的输出。<br>​        从数学上来定义，有限自动机是一个五元组:</p><script type="math/tex; mode=display">FA=(Q,S,\delta,q_0,F)</script><p>​        其中，$Q$是控制器的有限状态集、$S$是输入符号约有限集、$\delta$是控制状态转移规律的$Q×S$到$Q$的映射 (可用状态转移图或状态转移表表示)，$q_0$是初始状态、$F$是终止状态集。</p><p>​        若$\delta$是单值映射，则称$M$为确定性有限自动机;若$\delta$是多值映射，则称$M$为非确定性有限自动机。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;​        &lt;strong&gt;元胞自动机&lt;/strong&gt;(&lt;code&gt;Cellular Automata&lt;/code&gt;，简称CA，也有人译为细胞自动机、点格自动机、分子自动机或单元自动机)。是一时间和空间都离散的动力系统。散布在规则格网&lt;code&gt;(Lattice G</summary>
      
    
    
    
    
    <category term="元胞自动机" scheme="https://choeruii.github.io/tags/%E5%85%83%E8%83%9E%E8%87%AA%E5%8A%A8%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>回归方法</title>
    <link href="https://choeruii.github.io/2020/10/10/%E5%9B%9E%E5%BD%92%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://choeruii.github.io/2020/10/10/%E5%9B%9E%E5%BD%92%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2020-10-10T15:06:01.755Z</published>
    <updated>2021-01-25T09:34:52.823Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>线性回归——即对一组二元（单值）关系集的线性拟合，以寻找二元关系的函数表达。</p><p>简单线性回归模型如下：</p><script type="math/tex; mode=display">y = wx+b</script><p>其中$x$表示特征值，$w$表示权重，$b$表示偏置，$y$表示标签。</p><p><strong><em>多元线性回归</em></strong></p><p>对于一组$n$元关系（也可以认为是二元）的线性拟合。譬如：对于二元关系$(y,(x_1,x_2,…,x_n))$可以建立多元线性回归模型如下：</p><script type="math/tex; mode=display">y = b + w_1x_1+w_2x_2+...+w_nx_n</script><p>其中$x_i$表示第$i$个特征值，$w_i$表示第$i$个特征对应的权重，$b$表示偏置，$y$表示标签。</p><p>对线性回归模型，假设训练集中 m 个训练样本，每个训练样本中有 n 个特征，可以采用矩阵的表示方法，则多元线性回归模型还可以表示为：</p><script type="math/tex; mode=display">Y = \theta X</script><p>其中$\theta = (\theta_0,\theta_1,\theta_2,…,\theta_n)$。</p><p>其损失函数可表示为：</p><script type="math/tex; mode=display">𝐽(𝜃)=(𝑌−𝑌̂ )^2=(𝑌−𝜃𝑋)^=(𝑌−𝜃𝑋)𝑇(𝑌−𝜃𝑋)</script><p>其中，标签 𝑌 为 (m,1) 的矩阵，训练特征 𝑋 为 (m,n+1) 的矩阵(<strong>列数为n+1，是因为需要添加一列 $𝑥_0$， 并且这一列的值都为 1</strong>)，回归系数 𝜃 为 (n+1,1) 的矩阵，对 𝜃 求导，并令其导数等于 0 ，可以得到$X^T(Y−𝜃X)=0$。所以，最优解为：</p><script type="math/tex; mode=display">𝜃=(X^TX)^{-1}X^TY</script><p>这个就是正规方程解，我们可以通过最优方程解，直接求得我们所需要的参数 $𝜃$。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;线性回归模型&quot;&gt;&lt;a href=&quot;#线性回归模型&quot; class=&quot;headerlink&quot; title=&quot;线性回归模型&quot;&gt;&lt;/a&gt;线性回归模型&lt;/h2&gt;&lt;h3 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;</summary>
      
    
    
    
    
    <category term="拒绝废物指南" scheme="https://choeruii.github.io/tags/%E6%8B%92%E7%BB%9D%E5%BA%9F%E7%89%A9%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>深度学习小白白入门乱记</title>
    <link href="https://choeruii.github.io/2020/10/09/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>https://choeruii.github.io/2020/10/09/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</id>
    <published>2020-10-09T15:00:24.148Z</published>
    <updated>2020-10-10T15:04:46.865Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习主要分为三类：监督学习、非监督学习、强化学习</p><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>根据输出是连续还是离散的，把监督学习分为回归问题和分类问题。根据分类个数，还可分为两类分类问题或是多类分类问题。</p><p>监督学习本质上是一种知识的传递。</p><h3 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h3><p>非监督学习的训练数据只有输入而没有分类标签等输出。其目标是根据数据的分布发现数据的规律。例如当前比较热门的生成对抗网络<code>(GAN)</code>。</p><p>常见任务包括<strong>降维</strong>和<strong>聚类</strong>。</p><ul><li>降维的目的是把高维空间的点减低到一个低维空间上，同时尽可能的多保留信息。用途包括使可视化数据，<code>t-SNE</code>是一种很常见的用于可视化数据的降维技术。</li><li>聚类的目的是把相似的数据点放一起。由算法区分类别（标记）。</li></ul><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>获取新知识的唯一途径。（？？）</p><h3 id="常见的监督学习模型"><a href="#常见的监督学习模型" class="headerlink" title="常见的监督学习模型"></a>常见的监督学习模型</h3><h4 id="1-朴素贝叶斯分类器（Naive-Bayes-Classifier）"><a href="#1-朴素贝叶斯分类器（Naive-Bayes-Classifier）" class="headerlink" title="1.朴素贝叶斯分类器（Naive Bayes Classifier）"></a>1.朴素贝叶斯分类器（Naive Bayes Classifier）</h4><p>朴素贝叶斯分类器<code>(Naive Bayes Classifier)</code>非常简单，其假设样本的特征在给定分类的条件下是相互独立的。</p><p>特征<code>(Feature)</code>是被观察现象的一个可以测量的属性。好的特征让学习变的容易，无关的特征会干扰模型。</p><p>假设输入<strong>X</strong> = $(x_1,x_2,…,x_n)$，<code>NBC</code>会计算给定<strong>X</strong>条件下分类是$C_k$的概率$P(C_k|x_1,x_2,…,x_n)$。通过贝叶斯公式，可以得到：</p><script type="math/tex; mode=display">P(C_k|X) =P(C_k|x_1,x_2,...,x_n)=\frac{P(x_1,x_2,...,x_n|C_k)}{P(X)}</script><p>根据条件独立假设，给定了类别$C_k$之后各个特征$x_i$和$x_j$是独立的，因此上式可写为：</p><script type="math/tex; mode=display">\frac{\prod_iP(x_i|C_k)P(C_k)}{P(X)}</script><p>给定<strong>X</strong>，分母$P(X)$是一个固定的用于归一化的常量。（概率加起来等于一），可以忽略它。</p><p>在预测的时候只需知道$P(x_i|C_k)$和$P(C_k)$就可以计算出未归一化的$P(C_k|X)$，因此可以挑选概率最大的$k$作为预测的分类。</p><p>举例：用于垃圾邮件分类。</p><h4 id="2-逻辑回归（Logistic-Regression）"><a href="#2-逻辑回归（Logistic-Regression）" class="headerlink" title="2.逻辑回归（Logistic Regression）"></a>2.逻辑回归（Logistic Regression）</h4><p>逻辑回归是一个用于二值分类问题（binary classification problem）的算法。</p><p>将线性输出域范围的$(-\infty,+\infty)$映射到$(0,1)$区间上。此处须提到重要的$Sigmoid$函数：</p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{e^x+1}</script><p>$|x|\rightarrow 0$时，$\sigma(x)$近似线性函数$y = \frac{1}{4}x+\frac{1}{2}$。</p><p>应用举例：图像分类（Y or N），图像在计算机中由三个独立矩阵保存（R、G、B）,$64<em>64$。把像素亮度值提取出来放入一个特征向量（很长，维度 = $64</em>64*3$）</p><p>训练集（Notation）：输入样本<strong>X</strong> = $[x_1,x_2,…,x_n]$，$x_i（i=1,2，…,n）$为<strong>特征</strong>的值。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;机器学习主要分为三类：监督学习、非监督学习、强化学习&lt;/p&gt;
&lt;h3 id=&quot;监督学习&quot;&gt;&lt;a href=&quot;#监督学习&quot; class=&quot;headerlink&quot; title=&quot;监督学习&quot;&gt;&lt;/a&gt;监督学习&lt;/h3&gt;&lt;p&gt;根据输出是连续还是离散的，把监督学习分为回归问题和分类问</summary>
      
    
    
    
    
    <category term="拒绝废物指南" scheme="https://choeruii.github.io/tags/%E6%8B%92%E7%BB%9D%E5%BA%9F%E7%89%A9%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>拖延症小赵的第一篇blog</title>
    <link href="https://choeruii.github.io/2020/10/01/first-piece-of-blog/"/>
    <id>https://choeruii.github.io/2020/10/01/first-piece-of-blog/</id>
    <published>2020-10-01T06:55:07.202Z</published>
    <updated>2020-10-05T15:34:33.721Z</updated>
    
    
    
    
    
    <category term="拒绝废物指南" scheme="https://choeruii.github.io/tags/%E6%8B%92%E7%BB%9D%E5%BA%9F%E7%89%A9%E6%8C%87%E5%8D%97/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://choeruii.github.io/2020/10/01/hello-world/"/>
    <id>https://choeruii.github.io/2020/10/01/hello-world/</id>
    <published>2020-10-01T05:27:54.246Z</published>
    <updated>2020-10-01T05:27:54.246Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
